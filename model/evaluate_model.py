import os
import glob
import argparse

from PIL import Image
# https://pypi.org/project/ImageHash/
import imagehash


"""
Checks the output files from `generate_punks.py` and outputs some
quantitative metrics to evaluate the quality of the model, e.g. by comparing
the results in terms of image similarity to the original punks.

Requires PIL, imagehash 

@author pere
"""


def hash_image(img_path: str, method: str = 'average', hash_size=12) -> str:
    # implement the hashing methods of choice here
    img = Image.open(img_path)
    if method == 'average':
        hash_ = imagehash.phash(img, hash_size)
    else:
        raise ValueError(f'Unknown method: {method}')
    img.close()

    return str(hash_)  # important! return the string version of it


def get_all_punk_files(punks_folder: str, glob_by: str='punk*.png') -> list:
    return glob.glob(os.path.join(punks_folder, glob_by))


def hash_ground_truth(punks_folder: str) -> dict:
    ground_truth = {}
    for punk_file in get_all_punk_files(punks_folder):
        h_ = hash_image(punk_file)
        if h_ in ground_truth:
            # print(f'Hash collision! {punk_file} with {ground_truth[h_]}')
            pass
        else:
            ground_truth[h_] = []
        ground_truth[h_].append(punk_file)
    return ground_truth


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--generated_punks_folder", required=True,
                        help="Folder generated by `generate_punks.py'")
    parser.add_argument("--original_punks_folder", default='./punks',
                        help="Original punks folder (flat structure)")

    return parser.parse_args()


if __name__ == '__main__':
    args = parse_args()

    print(f'Hashing the original dataset in {args.original_punks_folder} ...')
    gt = hash_ground_truth(args.original_punks_folder)

    print(f'There are {len(gt)} buckets of really similar punks'
          ' in the ground truth of punks. '
          'If this number is lower than but close to 10.000 then the '
          'image hashing algorithm used should be fairly reliable.')

    n_collisions = 0
    n_new = 0
    new_punk_files = []

    gt_key_distro = list(sorted(gt.keys()))
    collisions_distro = []

    print('Evaluating the similarity of the punks in '
          f'{args.generated_punks_folder} against the original punks...')

    generated_hashes = []
    generated_hashes_level_2 = []

    for punk_file in get_all_punk_files(args.generated_punks_folder,
                                        'result*png'):
        h_ = hash_image(punk_file)
        generated_hashes.append(h_)
        generated_hashes_level_2.append(hash_image(punk_file, hash_size=8))

        if h_ in gt:
            n_collisions += 1
            collisions_distro.append(gt_key_distro.index(h_))
        else:
            n_new += 1
            new_punk_files.append(punk_file)

    print(f'Number of new punks in output folder: {n_new}, '
          f'number of collisions: {n_collisions}')

    print('Evaluating the intra-set similarity of the generated punks...')

    n_similar = len(generated_hashes) - len(set(generated_hashes))
    n_similar_2 = len(generated_hashes_level_2) -\
        len(set(generated_hashes_level_2))

    print(f'There are {n_similar} punks that look really similar to another '
          'punk in the generated punks by the model.')
    print(f'There are {n_similar_2} punks that look somehow similar to another '
          'punk in the generated punks by the model.')
    